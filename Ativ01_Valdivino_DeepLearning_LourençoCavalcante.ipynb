{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5512.078423,
      "end_time": "2024-11-13T14:55:06.046245",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-11-13T13:23:13.967822",
      "version": "2.6.0"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lourencocavalcante/CAP-421-3-Deep-Learning/blob/main/Ativ01_Valdivino_DeepLearning_Louren%C3%A7oCavalcante.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/logoinpe.png?raw=true\" width = 500 align=\"left\">](https://www.gov.br/inpe/pt-br)\n",
        "\n",
        "[<img src=\"https://github.com/lourencocavalcante/LogosINPE/blob/main/LogoCAP.png?raw=true\" width = 300 align=\"right\">](http://www.inpe.br/posgraduacao/cap/)"
      ],
      "metadata": {
        "id": "rDo2DxvpB8-3"
      },
      "id": "rDo2DxvpB8-3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAP-730 Aprendizado Profundo (*Deep Leraning*)**\n",
        "## **Detecção de mudanças (change detection - CD) usando rede neural convolucional e a base de dados Modified SYSU-CD**\n",
        "\n",
        "**Docentes:** *Dr. Valdivino Alexandre de Santiago Junior, Dr. Elcio Hideiti Shiguemori e Dr. Thales Sehn Körti*\n",
        "\n",
        "**Discente:** *Lourenço José Cavalcante Neto*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3d8zkhMCBIR"
      },
      "id": "Z3d8zkhMCBIR"
    },
    {
      "id": "280b7764",
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:37:46.210263Z",
          "iopub.execute_input": "2024-11-17T05:37:46.210672Z",
          "iopub.status.idle": "2024-11-17T05:37:46.216798Z",
          "shell.execute_reply.started": "2024-11-17T05:37:46.210633Z",
          "shell.execute_reply": "2024-11-17T05:37:46.215844Z"
        },
        "id": "280b7764",
        "papermill": {
          "duration": 9.007601,
          "end_time": "2024-11-13T13:23:26.558423",
          "exception": false,
          "start_time": "2024-11-13T13:23:17.550822",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2a3fc7d",
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib > /dev/null 2>&1\n",
        "\n",
        "!pip install efficientnet_pytorch segmentation-models-pytorch > /dev/null 2>&1\n",
        "\n",
        "!pip install -q kaggle > /dev/null 2>&1\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:37:46.221002Z",
          "iopub.execute_input": "2024-11-17T05:37:46.221586Z",
          "iopub.status.idle": "2024-11-17T05:38:23.859113Z",
          "shell.execute_reply.started": "2024-11-17T05:37:46.221549Z",
          "shell.execute_reply": "2024-11-17T05:38:23.857820Z"
        },
        "id": "b2a3fc7d",
        "papermill": {
          "duration": 44.127157,
          "end_time": "2024-11-13T13:24:10.690195",
          "exception": false,
          "start_time": "2024-11-13T13:23:26.563038",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9c12d274",
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Define o caminho do arquivo para download\n",
        "output_dir=\"/kaggle/working/modified-sysu-cd.zip\"\n",
        "\n",
        "# Verifica se o arquivo já foi baixado\n",
        "if [ -e \"$output_dir\" ]; then\n",
        "    echo \"O arquivo modified-sysu-cd.zip já está presente.\"\n",
        "else\n",
        "    # Realiza o download caso o arquivo não exista\n",
        "    kaggle datasets download valdivinosantiago/modified-sysu-cd\n",
        "fi\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:38:23.861653Z",
          "iopub.execute_input": "2024-11-17T05:38:23.862020Z",
          "iopub.status.idle": "2024-11-17T05:38:23.874468Z",
          "shell.execute_reply.started": "2024-11-17T05:38:23.861981Z",
          "shell.execute_reply": "2024-11-17T05:38:23.873540Z"
        },
        "id": "9c12d274",
        "outputId": "98e58f1c-d58f-4276-a1c0-4ac2e1735d81",
        "papermill": {
          "duration": 105.273213,
          "end_time": "2024-11-13T13:25:55.967854",
          "exception": false,
          "start_time": "2024-11-13T13:24:10.694641",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "O arquivo modified-sysu-cd.zip já está presente.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "7927df7e",
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define os caminhos do arquivo compactado e do destino da extração\n",
        "zip_path = \"/kaggle/working/modified-sysu-cd.zip\"\n",
        "extract_to = \"/kaggle/working/sample_data\"\n",
        "\n",
        "# Verifica se o diretório de destino já foi criado\n",
        "if not os.path.exists(extract_to):\n",
        "    # Extrai o conteúdo do arquivo zip, se ainda não extraído\n",
        "    with zipfile.ZipFile(zip_path, 'r') as archive:\n",
        "        archive.extractall(extract_to)\n",
        "else:\n",
        "    print(f\"O diretório {extract_to} já está disponível. Nenhuma ação necessária.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:38:23.876013Z",
          "iopub.execute_input": "2024-11-17T05:38:23.876388Z",
          "iopub.status.idle": "2024-11-17T05:38:23.885084Z",
          "shell.execute_reply.started": "2024-11-17T05:38:23.876344Z",
          "shell.execute_reply": "2024-11-17T05:38:23.884139Z"
        },
        "id": "7927df7e",
        "papermill": {
          "duration": 23.402758,
          "end_time": "2024-11-13T13:26:19.376026",
          "exception": false,
          "start_time": "2024-11-13T13:25:55.973268",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "outputId": "d2c183d8-ff49-4e7f-cc34-6242c5e54b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "O diretório /kaggle/working/sample_data já está disponível. Nenhuma ação necessária.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "18d72faf",
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Baixa o dataset especificado usando o comando Kaggle\n",
        "!kaggle datasets download -d valdivinosantiago/modified-sysu-cd\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:38:23.886408Z",
          "iopub.execute_input": "2024-11-17T05:38:23.886821Z",
          "iopub.status.idle": "2024-11-17T05:38:25.672132Z",
          "shell.execute_reply.started": "2024-11-17T05:38:23.886776Z",
          "shell.execute_reply": "2024-11-17T05:38:25.670982Z"
        },
        "id": "18d72faf",
        "outputId": "80158221-59bb-4698-f358-03f8f0ec263f",
        "papermill": {
          "duration": 3.35254,
          "end_time": "2024-11-13T13:26:22.734920",
          "exception": false,
          "start_time": "2024-11-13T13:26:19.382380",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset URL: https://www.kaggle.com/datasets/valdivinosantiago/modified-sysu-cd\nLicense(s): other\nmodified-sysu-cd.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "3c102ad8",
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomSYSUCDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, base_dir, image_transform=None, mask_transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        # Listar arquivos válidos nos diretórios\n",
        "        self.img_files = sorted([file for file in os.listdir(os.path.join(base_dir, 'time1'))\n",
        "                                 if os.path.isfile(os.path.join(base_dir, 'time1', file))])\n",
        "        self.mask_files = sorted([file for file in os.listdir(os.path.join(base_dir, 'label'))\n",
        "                                  if os.path.isfile(os.path.join(base_dir, 'label', file))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Caminhos completos para as imagens de T1, T2 e máscaras\n",
        "        img_t1_path = os.path.join(self.base_dir, 'time1', self.img_files[index])\n",
        "        img_t2_path = os.path.join(self.base_dir, 'time2', self.img_files[index])\n",
        "        mask_path = os.path.join(self.base_dir, 'label', self.mask_files[index])\n",
        "\n",
        "        # Carregar as imagens\n",
        "        img_t1 = Image.open(img_t1_path).convert('RGB')\n",
        "        img_t2 = Image.open(img_t2_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        # Aplicar as transformações, se definidas\n",
        "        if self.image_transform:\n",
        "            img_t1 = self.image_transform(img_t1)\n",
        "            img_t2 = self.image_transform(img_t2)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return img_t1, img_t2, mask\n",
        "\n",
        "\n",
        "# Definir transformações\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "mask_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Definir caminhos dos dados\n",
        "train_dir = '/kaggle/working/sample_data/trainmod'\n",
        "val_dir = '/kaggle/working/sample_data/valmod'\n",
        "test_dir = '/kaggle/working/sample_data/testmod'\n",
        "\n",
        "# Inicializar datasets\n",
        "train_data = CustomSYSUCDDataset(train_dir, image_transform=img_transforms, mask_transform=mask_transforms)\n",
        "val_data = CustomSYSUCDDataset(val_dir, image_transform=img_transforms, mask_transform=mask_transforms)\n",
        "test_data = CustomSYSUCDDataset(test_dir, image_transform=img_transforms, mask_transform=mask_transforms)\n",
        "\n",
        "batch_size = 6\n",
        "\n",
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:40:43.644519Z",
          "iopub.execute_input": "2024-11-17T05:40:43.644922Z",
          "iopub.status.idle": "2024-11-17T05:40:43.806060Z",
          "shell.execute_reply.started": "2024-11-17T05:40:43.644887Z",
          "shell.execute_reply": "2024-11-17T05:40:43.805173Z"
        },
        "id": "3c102ad8",
        "papermill": {
          "duration": 0.180738,
          "end_time": "2024-11-13T13:26:22.925618",
          "exception": false,
          "start_time": "2024-11-13T13:26:22.744880",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "687e47fd",
      "cell_type": "code",
      "source": [
        "def display_batch_shapes(loader):\n",
        "    for batch_idx, (image_t1, image_t2, label) in enumerate(loader):\n",
        "        print(f'Lote {batch_idx + 1}:')\n",
        "        print(f'Forma de image_t1: {image_t1.shape}')\n",
        "        print(f'Forma de image_t2: {image_t2.shape}')\n",
        "        print(f'Forma de label: {label.shape}')\n",
        "        print('---')\n",
        "\n",
        "        if batch_idx == 1:\n",
        "            break\n",
        "\n",
        "\n",
        "print(\"Formas de saída do conjunto de treinamento:\")\n",
        "display_batch_shapes(train_loader)\n",
        "\n",
        "print(\"Formas de saída do conjunto de validação:\")\n",
        "display_batch_shapes(val_loader)\n",
        "\n",
        "print(\"Formas de saída do conjunto de teste:\")\n",
        "display_batch_shapes(test_loader)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:40:52.232295Z",
          "iopub.execute_input": "2024-11-17T05:40:52.233309Z",
          "iopub.status.idle": "2024-11-17T05:40:52.509824Z",
          "shell.execute_reply.started": "2024-11-17T05:40:52.233266Z",
          "shell.execute_reply": "2024-11-17T05:40:52.508687Z"
        },
        "id": "687e47fd",
        "outputId": "16751169-7a24-4349-d42f-59d38a0dd74f",
        "papermill": {
          "duration": 0.427048,
          "end_time": "2024-11-13T13:26:23.360433",
          "exception": false,
          "start_time": "2024-11-13T13:26:22.933385",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Formas de saída do conjunto de treinamento:\nLote 1:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\nLote 2:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\nFormas de saída do conjunto de validação:\nLote 1:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\nLote 2:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\nFormas de saída do conjunto de teste:\nLote 1:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\nLote 2:\nForma de image_t1: torch.Size([6, 3, 256, 256])\nForma de image_t2: torch.Size([6, 3, 256, 256])\nForma de label: torch.Size([6, 1, 256, 256])\n---\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "3f9313bc",
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import models\n",
        "\n",
        "# U-Net com encoder ResNet34 pré-treinado\n",
        "class ResNet34UNet(nn.Module):\n",
        "    def __init__(self, in_channels=6, out_classes=1):\n",
        "        super(ResNet34UNet, self).__init__()\n",
        "\n",
        "        # ResNet34 pré-treinada como encoder\n",
        "        backbone = models.resnet34(pretrained=True)\n",
        "        backbone.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Reutilização do encoder da ResNet34, removendo camadas finais\n",
        "        self.encoder = nn.Sequential(*list(backbone.children())[:-2])\n",
        "\n",
        "        # Camadas de upsampling e decodificação\n",
        "        self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Camada final para ajuste de saída\n",
        "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
        "        self.final_layer = nn.Conv2d(128, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        encoded = self.encoder(input_data)\n",
        "        decoded = self.relu1(self.upconv1(encoded))\n",
        "        decoded = self.relu2(self.upconv2(decoded))\n",
        "        upsampled = self.upsample(decoded)\n",
        "        output = self.final_layer(upsampled)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Função para calcular a precisão binária\n",
        "def binary_accuracy(predictions, labels):\n",
        "    predictions = torch.sigmoid(predictions)  # Convertendo logits para probabilidade\n",
        "    predictions = (predictions > 0.5).float()  # Binarizando usando threshold de 0.5\n",
        "    matches = (predictions == labels).float().sum()\n",
        "    accuracy = matches / labels.numel()  # Total de elementos\n",
        "    return accuracy.item()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:40:58.673772Z",
          "iopub.execute_input": "2024-11-17T05:40:58.674200Z",
          "iopub.status.idle": "2024-11-17T05:40:58.688564Z",
          "shell.execute_reply.started": "2024-11-17T05:40:58.674161Z",
          "shell.execute_reply": "2024-11-17T05:40:58.687309Z"
        },
        "id": "3f9313bc",
        "papermill": {
          "duration": 0.020414,
          "end_time": "2024-11-13T13:26:23.386703",
          "exception": false,
          "start_time": "2024-11-13T13:26:23.366289",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c32e385a",
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Definir a classe do Dataset\n",
        "class ModifiedSYSUCDDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, label_transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        self.image_pairs = sorted([f for f in os.listdir(os.path.join(root_dir, 'time1')) if os.path.isfile(os.path.join(root_dir, 'time1', f))])\n",
        "        self.masks = sorted([f for f in os.listdir(os.path.join(root_dir, 'label')) if os.path.isfile(os.path.join(root_dir, 'label', f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        t1_path = os.path.join(self.root_dir, 'time1', self.image_pairs[idx])\n",
        "        t2_path = os.path.join(self.root_dir, 'time2', self.image_pairs[idx])\n",
        "        mask_path = os.path.join(self.root_dir, 'label', self.masks[idx])\n",
        "\n",
        "        t1_image = Image.open(t1_path).convert('RGB')\n",
        "        t2_image = Image.open(t2_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        if self.transform:\n",
        "            t1_image = self.transform(t1_image)\n",
        "            t2_image = self.transform(t2_image)\n",
        "        if self.label_transform:\n",
        "            mask = self.label_transform(mask)\n",
        "\n",
        "        return t1_image, t2_image, mask\n",
        "\n",
        "# Definir transformações para as imagens e máscaras\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "label_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Caminho do dataset\n",
        "root_dir_train = '/kaggle/working/sample_data/trainmod'\n",
        "root_dir_val = '/kaggle/working/sample_data/valmod'\n",
        "\n",
        "# Criar datasets para treino e validação\n",
        "train_dataset = ModifiedSYSUCDDataset(root_dir_train, transform=transform, label_transform=label_transform)\n",
        "valid_dataset = ModifiedSYSUCDDataset(root_dir_val, transform=transform, label_transform=label_transform)\n",
        "\n",
        "# Definir DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=6, shuffle=False)\n",
        "\n",
        "# Definir o modelo U-Net com ResNet34 como encoder\n",
        "class ResNet34UNet(nn.Module):\n",
        "    def __init__(self, in_channels=6, out_classes=1):\n",
        "        super(ResNet34UNet, self).__init__()\n",
        "\n",
        "        # Carregar a ResNet34 pré-treinada\n",
        "        resnet34 = models.resnet34(pretrained=True)\n",
        "        resnet34.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "        # Usar o resto da ResNet34 (encoder)\n",
        "        self.encoder = nn.Sequential(*list(resnet34.children())[:-2])  # Excluindo as camadas finais da ResNet\n",
        "\n",
        "        # Camadas de decodificação\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Upsampling final para garantir que a saída tenha o tamanho exato (256, 256)\n",
        "        self.final_upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
        "        self.final_conv = nn.Conv2d(128, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # Passar pelo decoder\n",
        "        x = self.decoder(x)\n",
        "        x = self.final_upsample(x)\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Função para calcular a acurácia binária\n",
        "def binary_accuracy(preds, targets):\n",
        "    preds = torch.sigmoid(preds)  # Aplicar sigmoid para obter probabilidade\n",
        "    preds = (preds > 0.5).float()  # Binarizar com threshold 0.5\n",
        "    correct = (preds == targets).float().sum()\n",
        "    accuracy = correct / (targets.numel())  # Total de elementos\n",
        "    return accuracy.item()\n",
        "\n",
        "# Função de treinamento e validação\n",
        "def train_and_validate(model, train_loader, valid_loader, num_epochs=25, model_save_path='/kaggle/working/best_model.pth'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Definindo a função de perda e o otimizador\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    highest_valid_accuracy = 0.0  # Melhor acurácia de validação registrada\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_accuracy = 0\n",
        "\n",
        "        # Laço de treinamento\n",
        "        for img_t1, img_t2, target_mask in tqdm(train_loader):\n",
        "            # Concatenar as imagens t1 e t2 ao longo do canal\n",
        "            input_images = torch.cat((img_t1, img_t2), dim=1).to(device)\n",
        "            target_mask = target_mask.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(input_images)\n",
        "            loss = loss_fn(predictions, target_mask)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            total_train_accuracy += binary_accuracy(predictions, target_mask)\n",
        "\n",
        "        # Calcular métricas de treinamento\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
        "\n",
        "        # Fase de validação\n",
        "        model.eval()\n",
        "        total_valid_loss = 0\n",
        "        total_valid_accuracy = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for img_t1, img_t2, target_mask in valid_loader:\n",
        "                input_images = torch.cat((img_t1, img_t2), dim=1).to(device)\n",
        "                target_mask = target_mask.to(device)\n",
        "\n",
        "                predictions = model(input_images)\n",
        "                loss = loss_fn(predictions, target_mask)\n",
        "\n",
        "                total_valid_loss += loss.item()\n",
        "                total_valid_accuracy += binary_accuracy(predictions, target_mask)\n",
        "\n",
        "        # Calcular métricas de validação\n",
        "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
        "        avg_valid_accuracy = total_valid_accuracy / len(valid_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, \"\n",
        "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {avg_valid_accuracy:.4f}\")\n",
        "\n",
        "        # Salvar o modelo com a melhor acurácia de validação\n",
        "        if avg_valid_accuracy > highest_valid_accuracy:\n",
        "            highest_valid_accuracy = avg_valid_accuracy\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(f\"Novo modelo salvo com acurácia de validação: {highest_valid_accuracy:.4f}\")\n",
        "\n",
        "    print(f\"Treinamento concluído. Melhor acurácia de validação: {highest_valid_accuracy:.4f}\")\n",
        "\n",
        "# Instanciar o modelo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet34UNet(in_channels=6, out_classes=1).to(device)\n",
        "\n",
        "# Treinamento do modelo\n",
        "train_and_validate(model, train_loader, valid_loader, num_epochs=50)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T05:41:53.174749Z",
          "iopub.execute_input": "2024-11-17T05:41:53.175171Z",
          "iopub.status.idle": "2024-11-17T07:14:33.476363Z",
          "shell.execute_reply.started": "2024-11-17T05:41:53.175135Z",
          "shell.execute_reply": "2024-11-17T07:14:33.475288Z"
        },
        "papermill": {
          "duration": 5246.130582,
          "end_time": "2024-11-13T14:53:49.522874",
          "exception": false,
          "start_time": "2024-11-13T13:26:23.392292",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "c32e385a",
        "outputId": "e98fe010-7f8b-4751-9132-617175f2c848"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/50], Train Loss: 0.4808, Train Accuracy: 0.8450, Valid Loss: 0.4317, Valid Accuracy: 0.8443\nNovo modelo salvo com acurácia de validação: 0.8443\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/50], Train Loss: 0.4111, Train Accuracy: 0.8689, Valid Loss: 0.6336, Valid Accuracy: 0.8633\nNovo modelo salvo com acurácia de validação: 0.8633\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/50], Train Loss: 0.3738, Train Accuracy: 0.8814, Valid Loss: 0.3862, Valid Accuracy: 0.8715\nNovo modelo salvo com acurácia de validação: 0.8715\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:42<00:00,  9.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/50], Train Loss: 0.3493, Train Accuracy: 0.8896, Valid Loss: 0.3611, Valid Accuracy: 0.9010\nNovo modelo salvo com acurácia de validação: 0.9010\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:42<00:00,  9.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/50], Train Loss: 0.3302, Train Accuracy: 0.8959, Valid Loss: 0.3307, Valid Accuracy: 0.8870\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:45<00:00,  9.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [6/50], Train Loss: 0.3132, Train Accuracy: 0.9019, Valid Loss: 0.3361, Valid Accuracy: 0.8865\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [7/50], Train Loss: 0.3027, Train Accuracy: 0.9037, Valid Loss: 0.3208, Valid Accuracy: 0.9065\nNovo modelo salvo com acurácia de validação: 0.9065\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [8/50], Train Loss: 0.2777, Train Accuracy: 0.9120, Valid Loss: 0.3435, Valid Accuracy: 0.9008\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [9/50], Train Loss: 0.2616, Train Accuracy: 0.9172, Valid Loss: 0.3580, Valid Accuracy: 0.8982\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.66it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [10/50], Train Loss: 0.2393, Train Accuracy: 0.9253, Valid Loss: 0.4312, Valid Accuracy: 0.8915\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:45<00:00,  9.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [11/50], Train Loss: 0.2157, Train Accuracy: 0.9331, Valid Loss: 0.3934, Valid Accuracy: 0.8813\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:42<00:00,  9.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [12/50], Train Loss: 0.1967, Train Accuracy: 0.9389, Valid Loss: 0.4622, Valid Accuracy: 0.8961\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:40<00:00,  9.95it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [13/50], Train Loss: 0.1890, Train Accuracy: 0.9419, Valid Loss: 0.4231, Valid Accuracy: 0.8995\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [14/50], Train Loss: 0.1578, Train Accuracy: 0.9507, Valid Loss: 0.4076, Valid Accuracy: 0.8834\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.83it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [15/50], Train Loss: 0.1429, Train Accuracy: 0.9555, Valid Loss: 0.5064, Valid Accuracy: 0.9023\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:42<00:00,  9.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [16/50], Train Loss: 0.1347, Train Accuracy: 0.9582, Valid Loss: 0.4602, Valid Accuracy: 0.8996\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [17/50], Train Loss: 0.1126, Train Accuracy: 0.9646, Valid Loss: 0.5259, Valid Accuracy: 0.8996\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [18/50], Train Loss: 0.1156, Train Accuracy: 0.9640, Valid Loss: 0.4955, Valid Accuracy: 0.8924\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [19/50], Train Loss: 0.1011, Train Accuracy: 0.9683, Valid Loss: 0.5133, Valid Accuracy: 0.9007\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [20/50], Train Loss: 0.0918, Train Accuracy: 0.9716, Valid Loss: 0.5170, Valid Accuracy: 0.9003\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [21/50], Train Loss: 0.0879, Train Accuracy: 0.9725, Valid Loss: 0.6103, Valid Accuracy: 0.8994\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [22/50], Train Loss: 0.0742, Train Accuracy: 0.9764, Valid Loss: 0.5834, Valid Accuracy: 0.9009\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [23/50], Train Loss: 0.0768, Train Accuracy: 0.9759, Valid Loss: 0.5986, Valid Accuracy: 0.9000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.55it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [24/50], Train Loss: 0.0801, Train Accuracy: 0.9749, Valid Loss: 0.6491, Valid Accuracy: 0.8986\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:44<00:00,  9.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [25/50], Train Loss: 0.0631, Train Accuracy: 0.9800, Valid Loss: 0.6698, Valid Accuracy: 0.9014\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.63it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [26/50], Train Loss: 0.0747, Train Accuracy: 0.9768, Valid Loss: 0.6781, Valid Accuracy: 0.8991\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [27/50], Train Loss: 0.0578, Train Accuracy: 0.9817, Valid Loss: 0.6682, Valid Accuracy: 0.8973\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:43<00:00,  9.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [28/50], Train Loss: 0.0581, Train Accuracy: 0.9817, Valid Loss: 0.5954, Valid Accuracy: 0.9007\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:42<00:00,  9.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [29/50], Train Loss: 0.0614, Train Accuracy: 0.9807, Valid Loss: 0.7069, Valid Accuracy: 0.9018\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [30/50], Train Loss: 0.0675, Train Accuracy: 0.9792, Valid Loss: 0.6402, Valid Accuracy: 0.9001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [31/50], Train Loss: 0.0507, Train Accuracy: 0.9842, Valid Loss: 0.6783, Valid Accuracy: 0.9005\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [32/50], Train Loss: 0.0437, Train Accuracy: 0.9862, Valid Loss: 0.6686, Valid Accuracy: 0.8970\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [33/50], Train Loss: 0.0411, Train Accuracy: 0.9868, Valid Loss: 0.8349, Valid Accuracy: 0.9024\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:40<00:00,  9.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [34/50], Train Loss: 0.0479, Train Accuracy: 0.9850, Valid Loss: 0.7527, Valid Accuracy: 0.8987\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:41<00:00,  9.80it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [35/50], Train Loss: 0.0525, Train Accuracy: 0.9837, Valid Loss: 0.8454, Valid Accuracy: 0.9010\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:40<00:00,  9.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [36/50], Train Loss: 0.0355, Train Accuracy: 0.9886, Valid Loss: 0.9143, Valid Accuracy: 0.9049\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:39<00:00, 10.01it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [37/50], Train Loss: 0.0340, Train Accuracy: 0.9891, Valid Loss: 0.9001, Valid Accuracy: 0.8992\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:39<00:00, 10.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [38/50], Train Loss: 0.0540, Train Accuracy: 0.9833, Valid Loss: 0.7759, Valid Accuracy: 0.8883\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.12it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [39/50], Train Loss: 0.0377, Train Accuracy: 0.9880, Valid Loss: 0.8784, Valid Accuracy: 0.9017\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [40/50], Train Loss: 0.0286, Train Accuracy: 0.9908, Valid Loss: 0.9762, Valid Accuracy: 0.9037\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [41/50], Train Loss: 0.0287, Train Accuracy: 0.9907, Valid Loss: 1.0094, Valid Accuracy: 0.9025\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.14it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [42/50], Train Loss: 0.0505, Train Accuracy: 0.9845, Valid Loss: 0.6498, Valid Accuracy: 0.8994\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [43/50], Train Loss: 0.0376, Train Accuracy: 0.9881, Valid Loss: 0.8878, Valid Accuracy: 0.9026\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [44/50], Train Loss: 0.0364, Train Accuracy: 0.9888, Valid Loss: 0.8285, Valid Accuracy: 0.8980\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.19it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [45/50], Train Loss: 0.0288, Train Accuracy: 0.9907, Valid Loss: 0.9354, Valid Accuracy: 0.9020\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [46/50], Train Loss: 0.0244, Train Accuracy: 0.9921, Valid Loss: 1.0115, Valid Accuracy: 0.9025\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:37<00:00, 10.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [47/50], Train Loss: 0.0380, Train Accuracy: 0.9883, Valid Loss: 0.8576, Valid Accuracy: 0.9003\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:37<00:00, 10.22it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [48/50], Train Loss: 0.0371, Train Accuracy: 0.9885, Valid Loss: 0.7835, Valid Accuracy: 0.8979\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:37<00:00, 10.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [49/50], Train Loss: 0.0289, Train Accuracy: 0.9908, Valid Loss: 0.8693, Valid Accuracy: 0.9030\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 1000/1000 [01:38<00:00, 10.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [50/50], Train Loss: 0.0311, Train Accuracy: 0.9904, Valid Loss: 0.9665, Valid Accuracy: 0.8967\nTreinamento concluído. Melhor acurácia de validação: 0.9065\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "a9ce84c2",
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/kaggle/working')\n",
        "!mkdir 'ChangeDetection_Valdivino'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T07:14:34.511309Z",
          "iopub.execute_input": "2024-11-17T07:14:34.511681Z",
          "iopub.status.idle": "2024-11-17T07:14:35.531909Z",
          "shell.execute_reply.started": "2024-11-17T07:14:34.511642Z",
          "shell.execute_reply": "2024-11-17T07:14:35.530802Z"
        },
        "papermill": {
          "duration": 3.091046,
          "end_time": "2024-11-13T14:53:54.725782",
          "exception": false,
          "start_time": "2024-11-13T14:53:51.634736",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "a9ce84c2",
        "outputId": "7e61463c-5291-460b-caa0-c00d80bc1621"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "mkdir: cannot create directory 'ChangeDetection_Valdivino': File exists\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "27f19e3d",
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "\n",
        "# Definir uma versão modificada do modelo U-Net com ResNet34 como backbone\n",
        "class ModifiedUNet(nn.Module):\n",
        "    def __init__(self, input_channels=6, output_classes=1):\n",
        "        super(ModifiedUNet, self).__init__()\n",
        "\n",
        "        # Carregar ResNet34 com pesos pré-treinados\n",
        "        base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "        base_model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Usar a parte inicial da ResNet (encoder)\n",
        "        self.encoder = nn.Sequential(*list(base_model.children())[:-2])\n",
        "\n",
        "        # Camadas do decoder (para reconstruir as dimensões da imagem)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=0),  # Expande para (128, 128)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0),  # Expande para (256, 256)\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Ajuste final de upsampling\n",
        "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
        "        self.final_conv = nn.Conv2d(128, output_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = self.upsample(x)\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "# Função para calcular o IoU (Intersection over Union)\n",
        "def compute_iou(predictions, targets, threshold=0.5):\n",
        "    # Binarizar as predições e os alvos\n",
        "    predictions = predictions > threshold\n",
        "    targets = targets > 0.5\n",
        "\n",
        "    # Flatten para comparação\n",
        "    predictions = predictions.view(predictions.size(0), -1)\n",
        "    targets = targets.view(targets.size(0), -1)\n",
        "\n",
        "    intersection = (predictions * targets).sum(dim=1)\n",
        "    union = (predictions + targets).sum(dim=1)\n",
        "\n",
        "    # Calcular a média do IoU para o batch\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou.mean().item()\n",
        "\n",
        "# Função para avaliar o modelo no conjunto de teste\n",
        "def model_evaluation(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_true_masks = []\n",
        "\n",
        "    total_time = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_t1, img_t2, mask in dataloader:\n",
        "            # Concatenar as imagens de diferentes tempos\n",
        "            input_images = torch.cat((img_t1, img_t2), dim=1).to(device)\n",
        "            true_mask = mask.to(device)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            # Obter a predição do modelo\n",
        "            outputs = model(input_images)\n",
        "\n",
        "            end = time.time()\n",
        "            total_time += (end - start) * 1000\n",
        "            batch_count += 1\n",
        "\n",
        "            # Armazenar predições e máscaras verdadeiras para métricas\n",
        "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
        "            all_true_masks.append(true_mask.cpu().numpy())\n",
        "\n",
        "    # Calcular a latência média\n",
        "    avg_latency = total_time / batch_count\n",
        "\n",
        "    # Flatten as predições e as máscaras\n",
        "    all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
        "    all_true_masks = np.concatenate(all_true_masks, axis=0).flatten()\n",
        "\n",
        "    # Calcular o IoU\n",
        "    iou = compute_iou(torch.tensor(all_preds > 0.5), torch.tensor(all_true_masks))\n",
        "    print(f\"IoU: {iou:.4f}\")\n",
        "\n",
        "    # Calcular o F1-score\n",
        "    f1 = f1_score(all_true_masks, all_preds > 0.5)\n",
        "\n",
        "    # Exibir métricas\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"Latência: {total_time:.4f} ms\")\n",
        "    print(f\"Latência média: {avg_latency:.4f} ms por batch\") total_time\n",
        "\n",
        "    return f1, iou, avg_latency\n",
        "\n",
        "# Carregar o modelo treinado\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ModifiedUNet(input_channels=6, output_classes=1).to(device)\n",
        "model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "f1, iou, avg_latency = model_evaluation(model, test_loader, device=device)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-17T07:22:33.953263Z",
          "iopub.execute_input": "2024-11-17T07:22:33.954206Z",
          "iopub.status.idle": "2024-11-17T07:23:37.323642Z",
          "shell.execute_reply.started": "2024-11-17T07:22:33.954161Z",
          "shell.execute_reply": "2024-11-17T07:23:37.322475Z"
        },
        "papermill": {
          "duration": 64.882747,
          "end_time": "2024-11-13T14:55:01.621186",
          "exception": false,
          "start_time": "2024-11-13T14:53:56.738439",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "27f19e3d",
        "outputId": "f3d9c508-070b-4589-af4b-be8ded06c013"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/3763316527.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "IoU: 0.9076\nF1-score: 0.7567\nLatência média: 5.3940 ms por batch\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}